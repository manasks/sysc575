{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SySc 575, ECE 4/555, AI: NN1, Fall 2017, Homework #1\n",
    "\n",
    "\n",
    "Treat this file as a lab notebook, and submit it - including code and written answers - upon completion. \n",
    "\n",
    "Give your written responses in ‘markdown’ cells after working out the code. We should be able to execute your code and get the same results that you did. It’s fine to include mistakes and the steps you went through to reach your results. When doing so, clearly mark the “final” code that corresponds to your written answers. Also, the more you annotate and describe your steps and choices, the better you’ll learn and the better we’ll be able to help you. (‘Markdown’ is easy to pick up, if you want to include basic formatting in those cells like I’ve done.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################################################################\n",
    "1. Single-layer XOR.\n",
    "-----------\n",
    "Adapt the sample code from HW0 to train a single perceptron on XOR data. Examine the resulting prediction, weights, and history, and comment on the success of the training.\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output data definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "out_data = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Layer Neural Network Model with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s\n",
      "[[ 0.5       ]\n",
      " [ 0.65112597]\n",
      " [ 0.75408173]\n",
      " [ 0.85125697]]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.predict_proba(in_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_history = []\n",
    "weight_history = []\n",
    "\n",
    "def capture_data(epoch,logs):\n",
    "    weights = model.get_weights()\n",
    "    weight_history.append(weights)\n",
    "    error_history.append(logs['loss'])\n",
    "    \n",
    "capture_data_callback = keras.callbacks.LambdaCallback(on_epoch_end=capture_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.8)\n",
    "model.compile(loss='mse', optimizer=sgd)\n",
    "hist = model.fit(in_data, out_data, batch_size=1, epochs=10000, verbose=0,callbacks=[capture_data_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s\n",
      "[[ 0.49528703]\n",
      " [ 0.462486  ]\n",
      " [ 0.45628721]\n",
      " [ 0.42389971]]\n",
      "[array([[-0.15644632],\n",
      "       [-0.1314861 ]], dtype=float32), array([-0.01885244], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_proba(in_data))\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the XOR data is not linearly separable in a two-dimensional weight space, a single neuron with 2-inputs cannot be reliably used for a XOR logic implementation.\n",
    "Varying the learning rate, number of epochs don't improve the accuracy of the trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################################################################\n",
    "2. XOR with a hidden layer of 2 nodes.\n",
    "-------------\n",
    "Repeat the training process with a multilayer perceptron, using two nodes in a hidden layer.\n",
    "* Identify the output weights, and describe what is happening with the hidden nodes and the output node.\n",
    "* Do the fit process at least a few more times, starting with new models and random seeds. What is different or the same about each of these experiments?\n",
    "* For two of the runs that appear substantially different in their weights, identify the logical function that each intermediary node is computing, as well as the output node. (For a network like this, with two inputs and one output per node, you should be able to write out a logic sentence or fill in a table of binary values.)\n",
    "\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output data definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "out_data = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Layer Neural Network Model with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation='sigmoid',kernel_initializer=keras.initializers.glorot_uniform(seed=348)))\n",
    "model.add(Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.glorot_uniform(seed=847)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s\n",
      "[[ 0.61063296]\n",
      " [ 0.61387116]\n",
      " [ 0.60049093]\n",
      " [ 0.60314268]]\n",
      "[array([[ 0.41417965, -0.61161131],\n",
      "       [-0.92264259,  0.71484089]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[ 0.36344996],\n",
      "       [ 0.53649718]], dtype=float32), array([ 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.predict_proba(in_data))\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_history = []\n",
    "weight_history = []\n",
    "\n",
    "def capture_data(epoch,logs):\n",
    "    weights = model.get_weights()\n",
    "    weight_history.append(weights)\n",
    "    error_history.append(logs['loss'])\n",
    "    \n",
    "capture_data_callback = keras.callbacks.LambdaCallback(on_epoch_end=capture_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.9, momentum=0.8)\n",
    "model.compile(loss='mse', optimizer=sgd)\n",
    "hist = model.fit(in_data, out_data, batch_size=1, epochs=10000, verbose=0,callbacks=[capture_data_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Probability:\n",
      "4/4 [==============================] - 0s\n",
      "[[ 0.00248843]\n",
      " [ 0.99785662]\n",
      " [ 0.99785644]\n",
      " [ 0.00225813]]\n",
      "Weights:\n",
      "[array([[ 7.05005074, -7.27228308],\n",
      "       [-7.25423193,  7.07936239]], dtype=float32), array([-3.84114361, -3.85897756], dtype=float32), array([[ 13.19900513],\n",
      "       [ 13.1932354 ]], dtype=float32), array([-6.54351759], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction Probability:\")\n",
    "print(model.predict_proba(in_data))\n",
    "print (\"Weights:\")\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Hidden Layer Seed: None, Output Layer Seed: None, Learning Rate: 0.6**<br>\n",
    "Prediction Probability:\n",
    "[[ 0.01052439]\n",
    " [ 0.99011272]\n",
    " [ 0.98792142]\n",
    " [ 0.00943174]]<br>\n",
    "Weights:\n",
    "[array([[-6.05538273, -6.64251518],\n",
    "       [ 6.28175402,  6.57256651]], dtype=float32), array([ 3.02769852, -3.55320954], dtype=float32), array([[-10.17666912],\n",
    "       [ 10.3923254 ]], dtype=float32), array([ 4.87385082], dtype=float32)]\n",
    "\n",
    "**2. Hidden Layer Seed: 15, Output Layer Seed: 87,Learning Rate: 0.6**<br>\n",
    "Prediction Probability:\n",
    "[[ 0.00963103]\n",
    " [ 0.98992419]\n",
    " [ 0.98997027]\n",
    " [ 0.01235967]]<br>\n",
    "Weights:\n",
    "[array([[-6.74988985, -5.14753294],\n",
    "       [-6.67412758, -5.13247728]], dtype=float32), array([ 2.79323983,  7.66390371], dtype=float32), array([[-10.84962273],\n",
    "       [ 10.70538902]], dtype=float32), array([-5.10975122], dtype=float32)]\n",
    "\n",
    "**3. Hidden Layer Seed: 348, Output Layer Seed: 847, Learning Rate: 0.6**<br>\n",
    "Prediction Probability:\n",
    "[[ 0.01146676]\n",
    " [ 0.99012995]\n",
    " [ 0.99012685]\n",
    " [ 0.0102966 ]]<br>\n",
    "Weights:\n",
    "[array([[ 6.18384171, -6.38674974],\n",
    "       [-6.37823915,  6.2049284 ]], dtype=float32), array([-3.41661978, -3.43107319], dtype=float32), array([[ 10.32640266],\n",
    "       [ 10.32269287]], dtype=float32), array([-5.10843992], dtype=float32)]\n",
    "\n",
    "**4. Hidden Layer Seed: 348, Output Layer Seed: 847, Learning Rate: 0.6, Momentum: 0.9**<br>\n",
    "Prediction Probability:\n",
    "[[ 0.00329878]\n",
    " [ 0.99715865]\n",
    " [ 0.99715841]\n",
    " [ 0.00298746]]<br>\n",
    "Weights:\n",
    "[array([[ 6.89499903, -7.10108995],\n",
    "       [-7.09406614,  6.90929937]], dtype=float32), array([-3.76589823, -3.77518201], dtype=float32), array([[ 12.67327595],\n",
    "       [ 12.67069435]], dtype=float32), array([-6.28166866], dtype=float32)]\n",
    "\n",
    "For Experiment 2, Logic function table:<br>\n",
    "Neuron 1<br>\n",
    "0 0 : 1<br>\n",
    "0 1 : 0<br>\n",
    "1 0 : 0<br>\n",
    "1 1 : 0<br>\n",
    "Neuron 2<br>\n",
    "0 0 : 1<br>\n",
    "0 1 : 1<br>\n",
    "1 0 : 1<br>\n",
    "1 1 : 0<br>\n",
    "Neuron 3<br>\n",
    "0 0 : 0<br>\n",
    "0 1 : 1<br>\n",
    "1 0 : 1<br>\n",
    "1 1 : 0<br>\n",
    "\n",
    "For Experiment 4, Logic function table:<br>\n",
    "Neuron 1<br>\n",
    "0 0 : 1<br>\n",
    "0 1 : 0<br>\n",
    "1 0 : 0<br>\n",
    "1 1 :<br>\n",
    "Neuron 2<br>\n",
    "0 0 : 1<br>\n",
    "0 1 : 1<br>\n",
    "1 0 : 1<br>\n",
    "1 1 : 0<br>\n",
    "Neuron 3<br>\n",
    "0 0 : 0<br>\n",
    "0 1 : 1<br>\n",
    "1 0 : 1<br>\n",
    "1 1 : 0<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################################################################\n",
    "3. Linearly separable logic with hidden layer\n",
    "---------------\n",
    "Train a network with the same structure as in Problem 2 on an easier logic relation of your choice – one that is linearly separable, and for which the hidden nodes are not necessary.\n",
    "* Before fitting the model, write down your predictions or expectations of how the training process might be different, and how the network will perform.\n",
    "* After training, see if you are able to interpret the weights, and what the hidden nodes are computing.\n",
    "* How did this experiment go relative to your predictions? Was there anything different about the training process?\n",
    "\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output data definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "out_data = np.array([[0],[0],[1],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Layer Neural Network Model with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation='sigmoid',kernel_initializer=keras.initializers.glorot_uniform(seed=874)))\n",
    "model.add(Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.glorot_uniform(seed=247)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s\n",
      "[[ 0.62251264]\n",
      " [ 0.66056901]\n",
      " [ 0.63159949]\n",
      " [ 0.66820812]]\n",
      "[array([[ 0.44191486,  0.17432195],\n",
      "       [-0.67715901,  0.59169042]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[-0.07099704],\n",
      "       [ 1.07145059]], dtype=float32), array([ 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.predict_proba(in_data))\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_history = []\n",
    "weight_history = []\n",
    "\n",
    "def capture_data(epoch,logs):\n",
    "    weights = model.get_weights()\n",
    "    weight_history.append(weights)\n",
    "    error_history.append(logs['loss'])\n",
    "    \n",
    "capture_data_callback = keras.callbacks.LambdaCallback(on_epoch_end=capture_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.5)\n",
    "model.compile(loss='mse', optimizer=sgd)\n",
    "hist = model.fit(in_data, out_data, batch_size=1, epochs=10000, verbose=0,callbacks=[capture_data_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s\n",
      "[[ 0.00948516]\n",
      " [ 0.00821553]\n",
      " [ 0.99332166]\n",
      " [ 0.99225444]]\n",
      "[array([[ 1.25975549,  6.69919395],\n",
      "       [-0.68281335, -0.18417738]], dtype=float32), array([-0.39594024, -3.24765706], dtype=float32), array([[  0.55842936],\n",
      "       [ 10.17581749]], dtype=float32), array([-5.2538352], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_proba(in_data))\n",
    "optimized_weights = model.get_weights()\n",
    "print(optimized_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The logic function that I have chosen for follows Input 1. It is lineraly separable and should therefore the model should perform quite well. Input 1 should see higher weights that Input 2.<br>\n",
    "\n",
    "**1. Hidden Layer Seed: 24, Output Layer Seed: 97**<br>\n",
    "Weights:<br>\n",
    "[array([[-5.66260767,  3.43532205],\n",
    "       [ 0.24899971, -0.06949561]], dtype=float32), array([ 2.40411735, -1.28243279], dtype=float32), array([[-8.20691872],\n",
    "       [ 4.54485273]], dtype=float32), array([ 1.43486977], dtype=float32)]<br>\n",
    "**2. Hidden Layer Seed: 874, Output Layer Seed: 247**<br>\n",
    "Weights:<br>\n",
    "[array([[ 1.25975549,  6.69919395],\n",
    "       [-0.68281335, -0.18417738]], dtype=float32), array([-0.39594024, -3.24765706], dtype=float32), array([[  0.55842936],\n",
    "       [ 10.17581749]], dtype=float32), array([-5.2538352], dtype=float32)]<br>\n",
    "       \n",
    "As we can see from the above set of final weights, Input 1 does not have a higher weight that Input 2.<br>\n",
    "Patterns I see from the weights: In hidden layer, Neuron 1 has higher BIAS weight thatn Neuron 2<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################################################################\n",
    "4. XOR in a larger network\n",
    "----------\n",
    "Create a new network structure to train on the XOR data. For instance, try using additional hidden nodes or another hidden layer. For any change you make, predict what the effect will be on the experiment relative to the outcomes in Problem 2. Analyze the results and compare them to your predictions.\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output data definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "out_data = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Layer Neural Network Model with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 6)                 36        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s\n",
      "[[ 0.6920554 ]\n",
      " [ 0.69246233]\n",
      " [ 0.69203466]\n",
      " [ 0.69245374]]\n",
      "[array([[ 0.40737545, -0.5712862 ,  0.87423062,  0.08292985],\n",
      "       [ 0.25969815,  0.7765348 ,  0.31058502, -0.03965157]], dtype=float32), array([ 0.,  0.,  0.,  0.], dtype=float32), array([[ 0.43444011,  0.02545284,  0.07216254,  0.1681428 ,  0.23281334],\n",
      "       [-0.0320204 , -0.22553203, -0.56678939, -0.67654502,  0.55166703],\n",
      "       [-0.39626595,  0.42309591,  0.27883247, -0.49541742, -0.41642454],\n",
      "       [ 0.62510431, -0.52138448, -0.67632407,  0.66383255,  0.10549933]], dtype=float32), array([ 0.,  0.,  0.,  0.,  0.], dtype=float32), array([[ 0.67739397, -0.6049841 ,  0.2472242 , -0.39656588,  0.62194192,\n",
      "        -0.07843837],\n",
      "       [-0.04310733, -0.21730174, -0.17157561, -0.26608673, -0.70117962,\n",
      "         0.42309475],\n",
      "       [ 0.44810277, -0.43516791,  0.7125597 , -0.09805582,  0.41819671,\n",
      "         0.17004138],\n",
      "       [-0.53172928,  0.24880621, -0.70076752, -0.08144624, -0.43123099,\n",
      "         0.22033057],\n",
      "       [ 0.19444598,  0.41957545, -0.66046846,  0.26691353, -0.6340794 ,\n",
      "         0.67320532]], dtype=float32), array([ 0.,  0.,  0.,  0.,  0.,  0.], dtype=float32), array([[ 0.8661254 ],\n",
      "       [-0.34567317],\n",
      "       [-0.38668641],\n",
      "       [ 0.03134051],\n",
      "       [ 0.57488334],\n",
      "       [ 0.52117872]], dtype=float32), array([ 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.predict_proba(in_data))\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_history = []\n",
    "weight_history = []\n",
    "\n",
    "def capture_data(epoch,logs):\n",
    "    weights = model.get_weights()\n",
    "    weight_history.append(weights)\n",
    "    error_history.append(logs['loss'])\n",
    "    \n",
    "capture_data_callback = keras.callbacks.LambdaCallback(on_epoch_end=capture_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.5)\n",
    "model.compile(loss='mse', optimizer=sgd)\n",
    "hist = model.fit(in_data, out_data, batch_size=1, epochs=10000, verbose=0,callbacks=[capture_data_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s\n",
      "[[ 0.48947161]\n",
      " [ 0.48980319]\n",
      " [ 0.48986641]\n",
      " [ 0.49010268]]\n",
      "[array([[ 0.6474157 , -0.58169234,  0.91257459,  0.22898449],\n",
      "       [ 0.53961569,  0.74498135,  0.4035387 ,  0.1236072 ]], dtype=float32), array([ 0.2045404 , -0.06186254,  0.00787427,  0.18724956], dtype=float32), array([[ 0.73662692,  0.46659914,  0.11046245,  0.65352261,  0.48172688],\n",
      "       [ 0.17069897,  0.11714319, -0.56289881, -0.28468245,  0.70778698],\n",
      "       [-0.0616717 ,  0.88323247,  0.32364142,  0.0049238 , -0.15383676],\n",
      "       [ 0.84545815, -0.16861406, -0.66328788,  1.07056141,  0.30567119]], dtype=float32), array([ 0.38340509,  0.63455564,  0.00930588,  0.74555218,  0.34990939], dtype=float32), array([[-0.21805637, -1.32631421, -0.39851546, -1.10291934, -0.04725088,\n",
      "        -1.0902828 ],\n",
      "       [-0.85824841, -0.95033211, -0.83005536, -0.95987159, -1.31712162,\n",
      "        -0.52558774],\n",
      "       [-0.05303051, -0.84969211,  0.32949066, -0.49902901,  0.04891429,\n",
      "        -0.40390792],\n",
      "       [-1.37277484, -0.46383783, -1.34923685, -0.7709741 , -1.06660235,\n",
      "        -0.74521798],\n",
      "       [-0.63646084, -0.24924596, -1.25389826, -0.38525397, -1.2558707 ,\n",
      "        -0.26659036]], dtype=float32), array([-1.39516044, -1.1300751 , -1.02076101, -1.09463084, -1.02793372,\n",
      "       -1.59209871], dtype=float32), array([[-0.10605375],\n",
      "       [-0.26526129],\n",
      "       [-0.16666116],\n",
      "       [-0.22520138],\n",
      "       [-0.13955249],\n",
      "       [-0.12787941]], dtype=float32), array([-0.02074067], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_proba(in_data))\n",
    "optimized_weights = model.get_weights()\n",
    "print(optimized_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "increasing the number of nodes in the hidden layer should improve accuracy.\n",
    "\n",
    "**Hidden Layer 1: 2 nodes**<br>\n",
    "[[ 0.01064747]\n",
    " [ 0.98884654]\n",
    " [ 0.98888898]\n",
    " [ 0.01369623]]<br>\n",
    "**Hidden Layer 1: 5 nodes**<br>\n",
    "[[ 0.00679121]\n",
    " [ 0.99107158]\n",
    " [ 0.99153799]\n",
    " [ 0.01047783]]<br>\n",
    "**Hidden Layer 1: 4 nodes**<br>\n",
    "[[ 0.0069016 ]\n",
    " [ 0.99095839]\n",
    " [ 0.99178982]\n",
    " [ 0.01030412]]<br>\n",
    "**Hidden Layer 1: 2 nodes, Hidden Layer 2: 4 n3des**<br>\n",
    "[[ 0.01026452]\n",
    " [ 0.99129486]\n",
    " [ 0.98990965]\n",
    " [ 0.00926109]]<br>\n",
    "**Hidden Layer 1: 5 nodes, Hidden Layer 2: 4 nodes, Hidden Layer 1: 1 nodes**<br>\n",
    "[[ 0.00999548]\n",
    " [ 0.98751265]\n",
    " [ 0.99064857]\n",
    " [ 0.00872787]]\n",
    "**Hidden Layer 1: 4 nodes, Hidden Layer 2: 5 nodes, Hidden Layer 1: 1 nodes**<br>\n",
    "[[ 0.00651652]\n",
    " [ 0.99047595]\n",
    " [ 0.99042261]\n",
    " [ 0.00914785]]\n",
    "\n",
    "\n",
    "As we see from the data above,increase in number of nodes per hidden layer increases the accuracy. But adding more hidden layers don't seem to have the same effect.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
